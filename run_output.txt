main() started
created network
created batch_gradient
created image_gradient
zeroed batch_gradient
zeroed image_gradient
zeroed network
After zero_network, before load_network
kernels file not found, will use initialized parameters (skipping load_kernels)
weights file not found, will use initialized parameters (skipping load_weights)
biases file not found, will use initialized parameters (skipping load_biases)
After load_network
Opened training files for iteration 0
Loss for batch(0, 0): 1.250000
Loss for batch(0, 1): 1.248999
Loss for batch(0, 2): 1.248001
Loss for batch(0, 3): 1.247006
Loss for batch(0, 4): 1.246008
Loss for batch(0, 5): 1.245008
Loss for batch(0, 6): 1.244020
Loss for batch(0, 7): 1.243029
Loss for batch(0, 8): 1.242036
Loss for batch(0, 9): 1.241046
Loss for batch(0, 10): 1.240058
Loss for batch(0, 11): 1.239070
Loss for batch(0, 12): 1.238082
Loss for batch(0, 13): 1.237098
Loss for batch(0, 14): 1.236110
Loss for batch(0, 15): 1.235140
Loss for batch(0, 16): 1.234148
Loss for batch(0, 17): 1.233172
Loss for batch(0, 18): 1.232195
Loss for batch(0, 19): 1.231214
Loss for batch(0, 20): 1.230237
Loss for batch(0, 21): 1.229263
Loss for batch(0, 22): 1.228283
Loss for batch(0, 23): 1.227320
Loss for batch(0, 24): 1.226334
Loss for batch(0, 25): 1.225373
Loss for batch(0, 26): 1.224411
Loss for batch(0, 27): 1.223439
Loss for batch(0, 28): 1.222476
Loss for batch(0, 29): 1.221506
Loss for batch(0, 30): 1.220543
Loss for batch(0, 31): 1.219582
Loss for batch(0, 32): 1.218621
Loss for batch(0, 33): 1.217661
Loss for batch(0, 34): 1.216711
Loss for batch(0, 35): 1.215750
Loss for batch(0, 36): 1.214798
Loss for batch(0, 37): 1.213829
Loss for batch(0, 38): 1.212865
Loss for batch(0, 39): 1.211934
Loss for batch(0, 40): 1.210956
Loss for batch(0, 41): 1.210014
Loss for batch(0, 42): 1.209055
Loss for batch(0, 43): 1.208142
Loss for batch(0, 44): 1.207165
Loss for batch(0, 45): 1.206238
Loss for batch(0, 46): 1.205303
Loss for batch(0, 47): 1.204331
Loss for batch(0, 48): 1.203401
Loss for batch(0, 49): 1.202469
Loss for batch(0, 50): 1.201520
Loss for batch(0, 51): 1.200578
Loss for batch(0, 52): 1.199662
Loss for batch(0, 53): 1.198722
Loss for batch(0, 54): 1.197783
Loss for batch(0, 55): 1.196831
Loss for batch(0, 56): 1.195896
Loss for batch(0, 57): 1.194987
Loss for batch(0, 58): 1.194057
Loss for batch(0, 59): 1.193150
Loss for batch(0, 60): 1.192191
Loss for batch(0, 61): 1.191249
Loss for batch(0, 62): 1.190351
Loss for batch(0, 63): 1.189426
Loss for batch(0, 64): 1.188513
Loss for batch(0, 65): 1.187576
Loss for batch(0, 66): 1.186681
Loss for batch(0, 67): 1.185750
Loss for batch(0, 68): 1.184804
Loss for batch(0, 69): 1.183914
Loss for batch(0, 70): 1.183018
Loss for batch(0, 71): 1.182048
Loss for batch(0, 72): 1.181160
Loss for batch(0, 73): 1.180279
Loss for batch(0, 74): 1.179320
Loss for batch(0, 75): 1.178431
Loss for batch(0, 76): 1.177511
Loss for batch(0, 77): 1.176613
Loss for batch(0, 78): 1.175733
Loss for batch(0, 79): 1.174795
Loss for batch(0, 80): 1.173930
Loss for batch(0, 81): 1.172981
Loss for batch(0, 82): 1.172127
Loss for batch(0, 83): 1.171187
Loss for batch(0, 84): 1.170289
Loss for batch(0, 85): 1.169383
Loss for batch(0, 86): 1.168495
Loss for batch(0, 87): 1.167616
Loss for batch(0, 88): 1.166705
Loss for batch(0, 89): 1.165795
Loss for batch(0, 90): 1.164937
Loss for batch(0, 91): 1.164051
Loss for batch(0, 92): 1.163167
Loss for batch(0, 93): 1.162274
Loss for batch(0, 94): 1.161357
Loss for batch(0, 95): 1.160500
Loss for batch(0, 96): 1.159621
Loss for batch(0, 97): 1.158746
Loss for batch(0, 98): 1.157848
Loss for batch(0, 99): 1.157003
Loss for batch(0, 100): 1.156101
Loss for batch(0, 101): 1.155175
Loss for batch(0, 102): 1.154346
Loss for batch(0, 103): 1.153459
Loss for batch(0, 104): 1.152563
Loss for batch(0, 105): 1.151701
Loss for batch(0, 106): 1.150842
Loss for batch(0, 107): 1.149972
Loss for batch(0, 108): 1.149090
Loss for batch(0, 109): 1.148229
Loss for batch(0, 110): 1.147367
Loss for batch(0, 111): 1.146522
Loss for batch(0, 112): 1.145615
Loss for batch(0, 113): 1.144764
Loss for batch(0, 114): 1.143903
Loss for batch(0, 115): 1.143059
Loss for batch(0, 116): 1.142148
Loss for batch(0, 117): 1.141352
Loss for batch(0, 118): 1.140491
Loss for batch(0, 119): 1.139613
Loss for batch(0, 120): 1.138770
Loss for batch(0, 121): 1.137880
Loss for batch(0, 122): 1.137029
Loss for batch(0, 123): 1.136171
Loss for batch(0, 124): 1.135303
Loss for batch(0, 125): 1.134472
Loss for batch(0, 126): 1.133630
Loss for batch(0, 127): 1.132778
Loss for batch(0, 128): 1.131925
Loss for batch(0, 129): 1.131042
Loss for batch(0, 130): 1.130252
Loss for batch(0, 131): 1.129456
Loss for batch(0, 132): 1.128545
Loss for batch(0, 133): 1.127757
Loss for batch(0, 134): 1.126875
Loss for batch(0, 135): 1.126047
Loss for batch(0, 136): 1.125220
Loss for batch(0, 137): 1.124373
Loss for batch(0, 138): 1.123498
Loss for batch(0, 139): 1.122699
Loss for batch(0, 140): 1.121834
Loss for batch(0, 141): 1.121036
Loss for batch(0, 142): 1.120220
Loss for batch(0, 143): 1.119379
Loss for batch(0, 144): 1.118509
Loss for batch(0, 145): 1.117781
Loss for batch(0, 146): 1.116879
Loss for batch(0, 147): 1.116047
Loss for batch(0, 148): 1.115295
Loss for batch(0, 149): 1.114397
Loss for batch(0, 150): 1.113550
Loss for batch(0, 151): 1.112743
Loss for batch(0, 152): 1.111963
Loss for batch(0, 153): 1.111105
Loss for batch(0, 154): 1.110310
Loss for batch(0, 155): 1.109491
Loss for batch(0, 156): 1.108658
Loss for batch(0, 157): 1.107888
Loss for batch(0, 158): 1.107063
Loss for batch(0, 159): 1.106184
Loss for batch(0, 160): 1.105462
Loss for batch(0, 161): 1.104657
Loss for batch(0, 162): 1.103800
Loss for batch(0, 163): 1.103074
Loss for batch(0, 164): 1.102177
Loss for batch(0, 165): 1.101342
Loss for batch(0, 166): 1.100582
Loss for batch(0, 167): 1.099741
Loss for batch(0, 168): 1.098958
Loss for batch(0, 169): 1.098177
Loss for batch(0, 170): 1.097369
Loss for batch(0, 171): 1.096591
Loss for batch(0, 172): 1.095817
Loss for batch(0, 173): 1.094924
Loss for batch(0, 174): 1.094230
Loss for batch(0, 175): 1.093447
Loss for batch(0, 176): 1.092568
Loss for batch(0, 177): 1.091769
Loss for batch(0, 178): 1.091061
Loss for batch(0, 179): 1.090217
Loss for batch(0, 180): 1.089435
Loss for batch(0, 181): 1.088686
Loss for batch(0, 182): 1.087883
Loss for batch(0, 183): 1.087148
Loss for batch(0, 184): 1.086309
Loss for batch(0, 185): 1.085502
Loss for batch(0, 186): 1.084736
Loss for batch(0, 187): 1.084046
Loss for batch(0, 188): 1.083170
Loss for batch(0, 189): 1.082364
Loss for batch(0, 190): 1.081627
Loss for batch(0, 191): 1.080875
Loss for batch(0, 192): 1.079971
Loss for batch(0, 193): 1.079316
Loss for batch(0, 194): 1.078517
Loss for batch(0, 195): 1.077678
Loss for batch(0, 196): 1.076995
Loss for batch(0, 197): 1.076209
Loss for batch(0, 198): 1.075448
Loss for batch(0, 199): 1.074687
Loss for batch(0, 200): 1.073892
Loss for batch(0, 201): 1.073103
Loss for batch(0, 202): 1.072403
Loss for batch(0, 203): 1.071600
Loss for batch(0, 204): 1.070778
Loss for batch(0, 205): 1.070142
Loss for batch(0, 206): 1.069337
Loss for batch(0, 207): 1.068517
Loss for batch(0, 208): 1.067776
Loss for batch(0, 209): 1.066979
Loss for batch(0, 210): 1.066186
Loss for batch(0, 211): 1.065585
Loss for batch(0, 212): 1.064742
Loss for batch(0, 213): 1.064030
Loss for batch(0, 214): 1.063230
Loss for batch(0, 215): 1.062535
Loss for batch(0, 216): 1.061753
Loss for batch(0, 217): 1.061042
Loss for batch(0, 218): 1.060303
Loss for batch(0, 219): 1.059506
Loss for batch(0, 220): 1.058740
Loss for batch(0, 221): 1.057996
Loss for batch(0, 222): 1.057263
Loss for batch(0, 223): 1.056613
Loss for batch(0, 224): 1.055876
Loss for batch(0, 225): 1.055093
Loss for batch(0, 226): 1.054343
Loss for batch(0, 227): 1.053581
Loss for batch(0, 228): 1.052821
Loss for batch(0, 229): 1.052111
Loss for batch(0, 230): 1.051263
Loss for batch(0, 231): 1.050594
Loss for batch(0, 232): 1.049868
Loss for batch(0, 233): 1.049253
Loss for batch(0, 234): 1.048464
Loss for batch(0, 235): 1.047743
Loss for batch(0, 236): 1.046946
Loss for batch(0, 237): 1.046259
Loss for batch(0, 238): 1.045459
Loss for batch(0, 239): 1.044822
Loss for batch(0, 240): 1.044062
Loss for batch(0, 241): 1.043373
Loss for batch(0, 242): 1.042665
Loss for batch(0, 243): 1.041944
Loss for batch(0, 244): 1.041169
Loss for batch(0, 245): 1.040465
Loss for batch(0, 246): 1.039699
Loss for batch(0, 247): 1.039061
Loss for batch(0, 248): 1.038388
Loss for batch(0, 249): 1.037566
Loss for batch(0, 250): 1.036899
Loss for batch(0, 251): 1.036142
Loss for batch(0, 252): 1.035492
Loss for batch(0, 253): 1.034754
Loss for batch(0, 254): 1.033985
Loss for batch(0, 255): 1.033281
Loss for batch(0, 256): 1.032721
Loss for batch(0, 257): 1.031888
Loss for batch(0, 258): 1.031262
Loss for batch(0, 259): 1.030521
Loss for batch(0, 260): 1.029706
Loss for batch(0, 261): 1.029042
Loss for batch(0, 262): 1.028346
Loss for batch(0, 263): 1.027634
Loss for batch(0, 264): 1.026926
Loss for batch(0, 265): 1.026238
Loss for batch(0, 266): 1.025561
Loss for batch(0, 267): 1.024896
Loss for batch(0, 268): 1.024261
Loss for batch(0, 269): 1.023515
Loss for batch(0, 270): 1.022804
Loss for batch(0, 271): 1.022116
Loss for batch(0, 272): 1.021410
Loss for batch(0, 273): 1.020659
Loss for batch(0, 274): 1.020055
Loss for batch(0, 275): 1.019311
Loss for batch(0, 276): 1.018684
Loss for batch(0, 277): 1.017959
Loss for batch(0, 278): 1.017301
Loss for batch(0, 279): 1.016643
Loss for batch(0, 280): 1.015886
Loss for batch(0, 281): 1.015255
Loss for batch(0, 282): 1.014524
Loss for batch(0, 283): 1.013782
Loss for batch(0, 284): 1.013160
Loss for batch(0, 285): 1.012492
Loss for batch(0, 286): 1.011790
Loss for batch(0, 287): 1.011035
Loss for batch(0, 288): 1.010363
Loss for batch(0, 289): 1.009725
Loss for batch(0, 290): 1.009052
Loss for batch(0, 291): 1.008401
Loss for batch(0, 292): 1.007700
Loss for batch(0, 293): 1.007064
Loss for batch(0, 294): 1.006419
Loss for batch(0, 295): 1.005685
Loss for batch(0, 296): 1.005108
Loss for batch(0, 297): 1.004417
Loss for batch(0, 298): 1.003672
Loss for batch(0, 299): 1.003065
Loss for iteration(0): 1.118208
Opened training files for iteration 1
Loss for batch(1, 0): 1.002288
Loss for batch(1, 1): 1.001654
Loss for batch(1, 2): 1.001011
Loss for batch(1, 3): 1.000447
Loss for batch(1, 4): 0.999731
Loss for batch(1, 5): 0.998944
Loss for batch(1, 6): 0.998436
Loss for batch(1, 7): 0.997818
Loss for batch(1, 8): 0.997054
Loss for batch(1, 9): 0.996477
Loss for batch(1, 10): 0.995720
Loss for batch(1, 11): 0.995180
Loss for batch(1, 12): 0.994465
Loss for batch(1, 13): 0.993725
Loss for batch(1, 14): 0.993095
Loss for batch(1, 15): 0.992555
Loss for batch(1, 16): 0.991848
Loss for batch(1, 17): 0.991110
Loss for batch(1, 18): 0.990577
Loss for batch(1, 19): 0.989865
Loss for batch(1, 20): 0.989224
Loss for batch(1, 21): 0.988555
Loss for batch(1, 22): 0.987891
Loss for batch(1, 23): 0.987352
Loss for batch(1, 24): 0.986531
Loss for batch(1, 25): 0.986007
Loss for batch(1, 26): 0.985367
Loss for batch(1, 27): 0.984713
Loss for batch(1, 28): 0.984080
Loss for batch(1, 29): 0.983441
Loss for batch(1, 30): 0.982787
Loss for batch(1, 31): 0.982181
Loss for batch(1, 32): 0.981535
Loss for batch(1, 33): 0.980916
Loss for batch(1, 34): 0.980318
Loss for batch(1, 35): 0.979648
Loss for batch(1, 36): 0.979084
Loss for batch(1, 37): 0.978317
Loss for batch(1, 38): 0.977605
Loss for batch(1, 39): 0.977150
Loss for batch(1, 40): 0.976367
Loss for batch(1, 41): 0.975814
Loss for batch(1, 42): 0.975105
Loss for batch(1, 43): 0.974675
Loss for batch(1, 44): 0.973916
Loss for batch(1, 45): 0.973379
Loss for batch(1, 46): 0.972779
Loss for batch(1, 47): 0.972048
Loss for batch(1, 48): 0.971452
Loss for batch(1, 49): 0.970879
Loss for batch(1, 50): 0.970206
Loss for batch(1, 51): 0.969598
Loss for batch(1, 52): 0.969052
Loss for batch(1, 53): 0.968430
Loss for batch(1, 54): 0.967791
Loss for batch(1, 55): 0.967089
Loss for batch(1, 56): 0.966424
Loss for batch(1, 57): 0.965942
Loss for batch(1, 58): 0.965323
Loss for batch(1, 59): 0.964820
Loss for batch(1, 60): 0.964048
Loss for batch(1, 61): 0.963368
Loss for batch(1, 62): 0.962889
Loss for batch(1, 63): 0.962269
Loss for batch(1, 64): 0.961684
Loss for batch(1, 65): 0.961017
Loss for batch(1, 66): 0.960519
Loss for batch(1, 67): 0.959849
Loss for batch(1, 68): 0.959139
Loss for batch(1, 69): 0.958656
Loss for batch(1, 70): 0.958111
Loss for batch(1, 71): 0.957293
Loss for batch(1, 72): 0.956810
Loss for batch(1, 73): 0.956311
Loss for batch(1, 74): 0.955503
Loss for batch(1, 75): 0.955013
Loss for batch(1, 76): 0.954362
Loss for batch(1, 77): 0.953803
Loss for batch(1, 78): 0.953299
Loss for batch(1, 79): 0.952580
Loss for batch(1, 80): 0.952109
Loss for batch(1, 81): 0.951333
Loss for batch(1, 82): 0.950954
Loss for batch(1, 83): 0.950189
Loss for batch(1, 84): 0.949605
Loss for batch(1, 85): 0.948995
Loss for batch(1, 86): 0.948447
Loss for batch(1, 87): 0.947896
Loss for batch(1, 88): 0.947267
Loss for batch(1, 89): 0.946603
^C